
<h1 align="center">Hi ðŸ‘‹, I'm Mustafa Panezai</h1>
<h3 align="center">Student and A passionate frontend developer from Pakistan</h3>
<img align ="center" width="350" src="https://cdn.dribbble.com/users/1162077/screenshots/3848914/programmer.gif">

<p align="left"> <img src="https://komarev.com/ghpvc/?username=mustafakhan3&label=Profile%20views&color=0e75b6&style=flat" alt="mustafakhan3" /> </p>

- ðŸŒ± Iâ€™m currently learning **java-script**

- âš¡ Fun fact **FUNNY**

<h3 align="left">Connect with me:</h3>
<p align="left">
</p>

<h3 align="left">Languages and Tools:</h3>
<p align="left"> <a href="https://www.w3schools.com/css/" target="_blank" rel="noreferrer"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/css3/css3-original-wordmark.svg" alt="css3" width="40" height="40"/> </a> <a href="https://www.w3.org/html/" target="_blank" rel="noreferrer"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/html5/html5-original-wordmark.svg" alt="html5" width="40" height="40"/> </a> <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript" target="_blank" rel="noreferrer"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/javascript/javascript-original.svg" alt="javascript" width="40" height="40"/> </a> </p>

<p><img align="center" src="https://github-readme-stats.vercel.app/api/top-langs?username=mustafakhan3&show_icons=true&locale=en&layout=compact" alt="mustafakhan3" /></p>


<p>&nbsp;<img  src="https://github-readme-stats.vercel.app/api?username=mustafakhan3&show_icons=true&locale=en" alt="mustafakhan3" /></p>


<h1>  Introduction to datascience </h1>
   <p> Data-science is a collection of  scientific methods and processes It is a combination of mathematics, statistics, computer science, and domain knowledge. data science is used to gain data about a specific topic .</p>
  <p> key elements of data science:</p>
  <li> statical analysis </li>
 <li>machine learning</li> <li>data visualization</li>  <li>data decision making </li>
                                                                             
<h1>  Importance of datascience </h1>
<p> â—¦ if we take from a small project.first we need the details of the project that we will be going to make .without any data we cannot make that project.so data science plays major role in any thing or research we have to study different theories and collect that data from them . </p>
<p> examples of industries that use data science is (health-care,finance,marketing and technology </p>
     <h1> Processes of Data-science </h1>  
     <p> <li> Data-collection </li> 
   <li> data cleansing and preprocessing </li>                                         
    <li> statical analysis and modeling </li>                                                         
    <li> maching learning and predictive modeling </li>                  
     <li> data visualization and communication </li>                                            
  <li> developing data-driven solutios to real world problems </li> 
  
  <h1> Data science Skills </h1>
  <p> these skills required to make career in data-science.
  <li> analytical and critical  thinking. </li>
    <li> problem-solving </li>
      <li> programming and data-manipulation </li>
        <li> statical knowledge and modeling </li>
          <li> maching learning and ai knowledge </li>
  </p>
  
   <h1> Career opportunities </h1>
  <p> in todays world if we see everything needs for starting anything they must have a background of this thing that they wil be making.
  <li> data-analyst </li>
    <li> data scientist </li>
      <li> maching learning engineer </li>
        <li> business inteligent analyst </li>
          <li> big data architect </li>
  </p>
  
  <h1> Conclusion </h1>
  <p> every project or work that we will be dealing with  a bunch of data to collect first and after that we can work on that data .data-science helos us to make our data collection method easy.by applying all the processes of data science things can be easy for making and research on specific project.
   
   # PYTHON PROGRAMMING LANGUAGE
   # Functions
   we have studied about the functions.for creating of functions we write def which means that it is user made
   and then we gave it the name we want ,braces then we have the choice that either we have parameters in it or make it empty and after that we use ':' this sign and then the main section comes here we write the code or logic we want to make.
   
   # Types and Sequences:
   Type is used to return the object type means that it tells us that whether it is int,float etc .in the sequences
   section we have started from tuples which is immutable datastructure how to create tuples.after that we have studied about the 
   list  which is a mutable data-structure.we studied about that how to show a spicific word location from there by using x[0] suppose.in that section we have also studied about append which is used to add an object to a list string or integer.
   we have also studied about the while how it can be used and how to perform different operation or add different logics form it.
   we have studied about concatenation of a list mean to combine to list together .also studied about the '*' sign means 
   to check whether a specific string or int is in the the list.in that topic we have also studied the slicing concept by using the bracket notation .we tell the staring index and the ending index if necessory and then it shows only those results.
   also in that section we studied about the the split concept which returns all the words in a string or it will remain on specific charachter.we studied also about the dictionaries.
In Python, a dictionary is a built-in data type that represents a collection of key-value pairs. It is also known as an associative array or a hash map in other programming languages. Dictionaries are mutable, unordered, and indexed containers.

Each element in a dictionary consists of two parts: a key and a corresponding value.
   # More on strings
   
String formatting in Python refers to the process of constructing strings by inserting values into predefined placeholders within the string. It allows you to create dynamic strings that incorporate variables, expressions, or other values.
   # Reading and Writting CSV files:
   suppose if we have a large dataset which is in excel.for importing that data into out code we use the csv concept .first we import it  the we give the location where the data is stored and then we dictreader which has read in each row of the csv file.len concepts tell that our list is made up of many dictionaries.keys concept gives us the column names of our csv.in that section we studied the 'set' which returns unique values for our class types in dataset.
   
   # Dates And Time:
   we can use the python dates and time for checking the current date and time.we import date and time from python libraries and by giving the specific function name we can find the date and time .
   for-example:
   date.today return the current date and alot more we can use.
   
   # Numpy
   We have studied the numpy packkage in python.this numpy helps us to create powerfull ways to create,store and manipulate our data.it makes it speedy to integrate with a wide variety of databases.also the foundation that pandas is built.numpy package helps us to create arrays ,selecting elements of arrays and loading the dataset into the array.
   # Array-Creation
   array can be diplayed like a list or a list of lists we pass in a list as argument in array.
   we can display the by first importing the numpy package and then we have to declare the variable we want and equal it to, and then square braces and the values and inserting it into it.
   for using multidimentsional array we have to use the double time square braces and passing the value into it.we can find the datatype the array elements by using the 'dtype' in it.by using shape attribute we can find the length of each dimension by calling the shape attribute.Arrays gives us several functions to create arrays by writting onews with it and giving the rows and columns names into it .array can be created which will be showing only the ones values in it.also we can use the zeros in it and zeros array can also be created .we can also create random arrays by writting random.rand in it and random array will be created.we can also create array with arrange function which by giving the starting index ending index and the difference between the two values it can be created which is time saving.if we want flooat values in array it can be created by using the linspace function and starting index ending index and the differnece between the two values.
   #Array-Operations
   we can do many array operation like the addition subtraction exponents and as well as boolean arrays,inverse etc.for addition we have to declare two arrays and by defining the variables and then using  '-' this minus inbetween the two variables subtraction we be done.multiplication can also be done by using the * sign  between the two declared array variables.if we want to do element wise manipulation we use the * sign and if we want to do product wise manipulation we use the @ sign.shape function will help us to find the inner dimensions of the array.we can also find the dtype of array by using the var name of array and writting the dtype with it.numpy arrays have many intresting agregation functions on them such as sum(),max(),min(),mean().sum used for addition and max used for finding the maximum number in the  array.min for minumber number in the array.
   # Indexing,Slicing and Iterating
   # Indexing
   first lets look at integer indexing. to get an element in one dimensional array we use the offset value a[2[.
   but for multidimensional array we use integer array indexing for finding certain value in multidimensional array we have to use indexing like a[1,1] wher first shows the rows and second shows the column of which one to select.if we want to select multiple elements from array we can enter the indices directly into an array function.
   # Boolean Indexing
   boolean indexing allows us to select arbitrary elements based on conditions .if we want to get elements greater than 5 we will use the greater sigh a>5 .this will help us that the the value should be greater than 5.
   
   # Slicing
   slicing is a way to create a subarray based on the original array .to slice we use the : this sign .for examole if we want elements starting from index 2 and ending on index 4 we will use this command a[2:4].if we add another argument to the array a[:2,1:3] we get the first two rows and then the second and third column values only.
   
  # Trying Numpy with datasets
   we have learned the essentials of numpy lets use it on a couplle of datasets.to load a dataset in numpy we will use the genfromtxt we can also spicify datafile name ,delimeter.inside of the genfromtext bracket we have to give the location of the dataset. If we want a range of columns in order, say columns 0 through 3 (recall, this means first, second, and
 third, since we start at zero and don't include the training index value), we can do that too
wines[:, 0:3]. What if we want several non-consecutive columns? We can place the indices of the columns that we want into
 an array and pass the array as the second argument. Here's an example
wines[:, [0,2,4]]. Recall boolean masking. We can use this to find out how many students have had research experience by
 creating a boolean mask and passing it to the array indexing operator
len(graduate_admission[graduate_admission['Research'] == 1])
   
   # Pandas Introduction
   lets look at the way that how to python can be used to manipulate clean and also query data by talking pandas toolkit.pandas created by wes Mckinny in 2008.pandas is a open source project under under a very permissive license.as an open source pandas has got a strong community.with many software developers all making code to help make it better.before pandas we have a hodge podge of tools numpy,the python core libraries.i want to introduce stack overflow pandas uses it as the number one resource for helping new members .if you post a question related to python or pandas the pandas community will respond and the will be given to your question.second resource you can use is that the book.wes mckinny wrote the book called pyrhon for data analysis and published by o'reilly and its recently updated to second verion.i consider this book to look for it.
   # The Series DataStructure
   Now we are going to explore the Series datastructure.we will be doing how to manipulate data ,store data in signle dimensional indexed data in the series object.an easy way to visualize is is two columns of data .the first is the special index and the second is the actual data . data column has has the label of its own and can be used using the .name attribute.
                                                                                                                  
   Now let's make a list of three students alice,jack and molly.
   lets import pandas
   import pandas as pd
   students = ['alice','jack','molly']
   now we just have to call the series 
   pd.series(students)
   
   we see that the values are indexed with integers starting at zero.also the result is a series object and is nicely rendered to the screen .pandas has atomatically identified the type of data in the series as object..
   if we passed in a list of whole numbers we see that pandas sets the type to int34.
   for example=
   numbers = [1,2,3]
   turn it into series 
   pd.series(numbers)
the result we see here is a type of int64 objects.
   there is some other details that exit is that how pandas and python handle missing data.in python we can have a none type to indicate lack of data.consider if we have a string series of list and we add one none value in it then pandas inserts is as a none type and the type object for it.
   4students=['alice','jack','mustafa',none]
pd.series(students)
   if wwe create a list of integers and floats pandas converts this into floating point value known as nan standas for not a number see the below example
   numbers=[1,2,None]
   pd.Series(numbers)
   output is nan and the dtype is float64.
   if we try equality test nan is not equal to none 
   np.nan==np.none the result is false
if we do the equality test to itselt it will result in false.
   np.nan== np.nan
   
   instead we have to use special functions to test the presence of not a number 
   np.isnan(np.nan)
   both meaning is similar but its numeric value and treated differently.
   we can also create a series with dictionary and the index will be automatically assigned to it.
   students_score = {'alice':'physics',
                      'jack' :'physics'}
   s=pd.series(students_scores)
   
   # The Data-Frame
   it is the heart of pandas library it's work is to data-analysis and cleaning tasks.it's a two dimensional series object where there is an index and multiple columns of content. \n 
   import pandas as pd
students = [{'Name': 'Alice',
              'Class': 'Physics',
              'Score': 85},
            {'Name': 'Jack',
             'Class': 'Chemistry',
             'Score': 82},
            {'Name': 'Helen',
             'Class': 'Biology',
             'Score': 90}]
   one method is that you could use a list of dictionaries, where each dictionary 
represents a row of data.



 Then we pass this list of dictionaries into the DataFrame function
df = pd.DataFrame(students, index=['school1', 'school2', 'school1'])
 And lets print the head again
df.head()
   by using the df.head() helps us to show several rows of the dataframe.
   we can etract the data using the loc and iloc attribute.passing a single value to the loc will return the series if there is only one row to return.
   if we wanted data associated with school2 we can just use the .loc attribute with one paramenter.
   df.loc["school"} suppose.# What would we do if we just wanted to select a single column though? Well, there are a few
 mechanisms. Firstly, we could transpose the matrix. This pivots all of the rows into columns
 and all of the columns into rows, and is done with the T attribute "df.T". Since the result of using the indexing operator is either a DataFrame or Series, you can chain 
 operations together. For instance, we can select all of the rows which related to school1 using
 .loc, then project the name column from just those rows
df.loc['school1']['Name'].the .loc alos supports slicing.If we wanted to select all rows, we can use a colon to indicate a full slice from beginning to end. df.loc[:,['Name', 'runs']].let's talk about dropping data.it's easy to delete data in series and dataframes we can use the drop function to drop data from it.df.drop('school1').we can make the copy of the dataframe using .copy() function.copy_df = df.copy().we can also add a new colomn to the dataframe is as easy as assigning it to the value using the indexing operator.df['ClassRanking'] = None
# Fetching Data Through API

This code snippet utilizes the pandas library for movie data analysis. It begins by importing the pandas and requests libraries using the command import pandas as pd and import requests, respectively. Then, a GET request is made to the Movie Database API to retrieve a list of top-rated movies. The response is stored in the response variable through the command response = requests.get('https://api.themoviedb.org/3/movie/top_rated?api_key=8265bd1679663a7ea12ac168da84d2e8&language=en-US&page=1').

Next, the response data is converted into a DataFrame using pandas with the command temp_df = pd.DataFrame(response.json()['results']). This DataFrame consists of columns like 'id', 'title', 'overview', 'release_date', 'popularity', 'vote_average', and 'vote_count'. The specific columns are selected using double square brackets [['id','title','overview','release_date','popularity','vote_average','vote_count']].

Finally, to display the first few rows of the DataFrame, the code temp_df.head() should be used. However, in the given paragraph, there is a mistake where df.head() is used instead of temp_df.head(). The correct command to display the first few rows of the DataFrame should be temp_df.head(). In summary, this code fetches movie data from a web API, converts it into a DataFrame, selects specific columns, and displays the initial rows of the DataFrame for analysis or visualization.
In this code, we begin by creating an empty DataFrame using the command df = pd.DataFrame(). Then, a loop is initiated using the range function from 1 to 429 (exclusive). Within each iteration, a GET request is made to the Movie Database API to retrieve a list of top-rated movies for a specific page. The page number is inserted into the URL using the .format(i) method. The response is stored in the response variable.

Similar to before, the response data is converted into a DataFrame named temp_df. This DataFrame consists of columns like 'id', 'title', 'overview', 'release_date', 'popularity', 'vote_average', and 'vote_count', which are selected using double square brackets [['id','title','overview','release_date','popularity','vote_average','vote_count']].

The df.append(temp_df, ignore_index=True) command appends the data from temp_df to the existing DataFrame df, with ignore_index=True ensuring the index is reset for the combined DataFrame.


# Pivot Tables
In this code, we use the pandas library to work with a university ranking dataset. After loading the dataset into a DataFrame, we create a new column called 'Rank_Level' based on the 'world_rank' column. We categorize universities into tiers based on their rankings using if/elif statements.

Next, we create a pivot table to compare rank levels with countries, focusing on the overall scores. The pivot table aggregates the mean score for each country and rank level using the pivot_table() function. We observe a hierarchical DataFrame where countries are the rows, rank levels are the columns, and the mean scores are the values. NaN values indicate that some countries only have observations in the "Other Top Universities" category.

To expand the pivot table, we can include multiple aggregation functions by passing a list of functions to the aggfunc parameter. In this case, we add the np.max function to calculate the maximum score for each country and rank level.
In this code, we further explore the pivot table created earlier. By setting margins=True in the pivot_table() function, we include an additional row and column called "All" that shows the overall mean and maximum scores.

The resulting pivot table, stored in the new_df DataFrame, has a multi-level structure. We can access series or cells in the pivot table similar to a regular DataFrame. For example, to retrieve the average scores of First Tier Top Universities in each country, we use new_df['mean']['First Tier Top University']. This will return a series object containing the average scores.

By applying the idxmax() function to the series, we can find the country with the maximum average score for First Tier Top Universities using new_df['mean']['First Tier Top University'].idxmax(). The idxmax() function retrieves the index label corresponding to the maximum value.

It's important to note that idxmax() is not specific to pivot tables but is a built-in function for Series objects. The pandas library provides a wide range of functions and attributes to explore further for data manipulation and analysis.

If you want to reshape the pivot table, the stack and unstack functions are available. Stacking pivots the lowest column index to become the innermost row index, while unstacking is the inverse operation, pivoting the innermost row index to become the lowest column index.

Overall, the pivot table provides a versatile way to analyze data across multiple dimensions, and by leveraging pandas functions and methods, we can extract specific information from the pivot table for further analysis or exploration.
# Scales
In this code, we work with categorical data in pandas.

We start by creating a DataFrame with letter grades and an index indicating students' performance. Initially, the grades are treated as simple text strings. To work with them more effectively, we convert the 'Grades' column to a special categorical data type using the astype() function.

By converting the grades to an ordered categorical type, we establish a natural ordering. This allows for correct comparisons and Boolean masking. We demonstrate this by comparing grades using both lexicographical comparison and the categorical comparison, which produces the expected results.

Next, we explore the concept of dummy variables. These are binary columns that indicate the presence or absence of a category. The get_dummies() function in pandas converts a single column into multiple columns of zeros and ones, making it useful for feature extraction.

Additionally, we discuss converting interval or ratio data into categorical data. The cut() function in pandas is used to divide data into equal-sized bins. We apply this to average county population data by state, generating categorical labels representing the states' average county sizes.
# Merging DataFrames
First, we discuss merging data horizontally using the merge() function. We use a Venn diagram to understand the concepts of union and intersection. In pandas, merging horizontally is done by specifying the left and right DataFrames and indicating the join type. We explore different types of joins such as inner join, outer join, left join, and right join, depending on the desired data combination. The merge operation can be performed based on index values or specific columns.

Next, we move on to concatenating data vertically using the concat() function. We consider an example where we have separate CSV files representing data for different years. We create individual DataFrames for each year and store them in a list. By passing the list to the concat() function, we vertically concatenate the DataFrames, combining the data from all years. This allows us to stack the dataframes on top of each other.

We discuss the option of using keys to maintain the identification of data from different sources. By setting the keys parameter in concat(), we assign an additional level of indices that corresponds to each DataFrame's source.

It is important to note that merging and concatenating datasets can result in NaN values when columns don't align or when data is missing. We mention the inner and outer methods, where inner removes NaN values by dropping observations, while outer retains NaN values in the resulting DataFrame.
# GroupBy
The groupby() function in pandas allows us to perform operations on data based on groups. It follows a split-apply-combine approach, where the data is split into groups, operations are applied to each group, and the results are combined.

To split the data, we specify the column or columns we want to group by. For example, we can group data by states in a census dataset. The groupby() function creates a GroupBy object that represents the grouped data.

Next, we can apply different operations to the groups. One common operation is aggregation, where we calculate summary statistics for each group. For example, we can find the average population of counties in each state. We use the agg() method on the GroupBy object and pass a dictionary specifying the columns and functions we want to apply.

Another operation is transformation, where we apply a function to each group and create a new DataFrame with the same shape as the original. This allows us to perform calculations within each group. We use the transform() method on the GroupBy object to achieve this.

We can also filter groups based on certain conditions using the filter() function. This allows us to include or exclude groups based on specific criteria.

Finally, we have the apply() function, which allows us to apply custom functions to each group. This gives us more flexibility in performing operations on the grouped data.

Using groupby(), we can efficiently analyze data on a group level without having to iterate over individual rows, which can be slow and inefficient.

In summary, the groupby() function in pandas provides a powerful tool for grouping data, applying operations, and obtaining aggregated or transformed results on a group level. It simplifies the process of analyzing and summarizing data based on specific categories or criteria.
   # Basic Understanding of data
   pd.read_csv('datasets/titanic.csv'): This command reads a CSV file named 'titanic.csv' and loads it into a pandas DataFrame called 'df'. The DataFrame is a tabular data structure that allows you to work with the data in a convenient way.

df.shape: This command returns the dimensions of the DataFrame, i.e., the number of rows and columns. It gives you an idea of how many data points and variables are present in the dataset.

df.info(): This command provides information about the DataFrame, including the column names, the data type of each column, and the number of non-null values. It also gives you an overview of the memory usage.

df.isnull().sum(): This command calculates the number of missing values in each column of the DataFrame. It returns a series that shows the count of null values for each column. It helps identify which columns have missing data.

df.duplicated().sum(): This command counts the number of duplicated rows in the DataFrame. Duplicated rows are those that have identical values across all columns. It helps identify and handle duplicate records in the dataset.

df.corr(): This command calculates the correlation between the numeric columns in the DataFrame. Correlation measures the strength and direction of the linear relationship between two variables. The result is a correlation matrix, which shows the correlation coefficients between each pair of numeric columns. It helps identify relationships and dependencies between variables.
# Univariate Analysis
pd.read_csv('datasets/titanic.csv'): This command reads a CSV file named 'titanic.csv' and loads it into a pandas DataFrame called 'df'. The DataFrame is a tabular data structure that allows you to work with the data in a convenient way.

df.shape: This command returns the dimensions of the DataFrame, i.e., the number of rows and columns. It gives you an idea of how many data points and variables are present in the dataset.

df.info(): This command provides information about the DataFrame, including the column names, the data type of each column, and the number of non-null values. It also gives you an overview of the memory usage.

df.isnull().sum(): This command calculates the number of missing values in each column of the DataFrame. It returns a series that shows the count of null values for each column. It helps identify which columns have missing data.

df.duplicated().sum(): This command counts the number of duplicated rows in the DataFrame. Duplicated rows are those that have identical values across all columns. It helps identify and handle duplicate records in the dataset.

df.corr(): This command calculates the correlation between the numeric columns in the DataFrame. Correlation measures the strength and direction of the linear relationship between two variables. The result is a correlation matrix, which shows the correlation coefficients between each pair of numeric columns. It helps identify relationships and dependencies between variables.

df = df.where(pd.notnull(df['Embarked']), axis=0).dropna(axis=0): This command replaces missing values in the 'Embarked' column with the value 'NaN' and then removes rows that contain any missing values in the DataFrame. It helps clean the data by handling missing values.

df = df.drop("Cabin", axis=1): This command removes the 'Cabin' column from the DataFrame. It helps eliminate a column that may not be relevant for analysis.

df["Parch"].unique(): This command returns an array of unique values in the 'Parch' column. It helps identify the different categories or levels present in that column.

sns.countplot(df['Embarked']): This command creates a countplot using the 'Embarked' column from the DataFrame. It shows the count of occurrences for each category in a categorical variable.

df['Survived'].value_counts().plot(kind='bar'): This command creates a bar plot to visualize the count of values in the 'Survived' column. It helps understand the distribution or frequency of a binary variable.

df['Sex'].value_counts().plot(kind='pie', autopct='%.2f'): This command creates a pie chart to visualize the proportions of different categories in the 'Sex' column. It helps show the relative frequencies of categorical data.


# Bivariate analysis

Importing libraries:

import pandas as pd: Imports the pandas library and assigns it the alias pd.
import seaborn as sns: Imports the seaborn library and assigns it the alias sns.
Loading datasets:

tips = sns.load_dataset('tips'): Loads the 'tips' dataset from the seaborn library into a pandas DataFrame called tips.
titanic = pd.read_csv('datasets/titanic.csv'): Reads a CSV file called 'titanic.csv' into a pandas DataFrame called titanic.
flights = sns.load_dataset('flights'): Loads the 'flights' dataset from the seaborn library into a pandas DataFrame called flights.
iris = sns.load_dataset('iris'): Loads the 'iris' dataset from the seaborn library into a pandas DataFrame called iris.
Scatter Plot:

sns.scatterplot(...): Creates a scatter plot using the 'total_bill' column as the x-axis, the 'tip' column as the y-axis, differentiating points by 'smoker' using color, differentiating points by 'sex' using different marker styles, and scaling the points by 'size'.
Bar Plot:

sns.barplot(...): Creates a bar plot using the 'Pclass' column as the x-axis, the 'Age' column as the y-axis, differentiating bars by 'Sex' using color.
Box Plot:

sns.boxplot(...): Creates a box plot using the 'Sex' column as the x-axis, the 'Age' column as the y-axis, differentiating boxes by 'Survived' using color.
Distribution Plots:

sns.distplot(...): Creates a distribution plot (combining histogram and kernel density estimate) for the 'Age' column, separating the data based on whether 'Survived' is 0 or 1.
sns.displot(...): Creates a distribution plot (kernel density estimate) for the 'Age' column, separating the data based on whether 'Survived' is 0 or 1. Additionally, the 'Pclass' column is used to show different distributions using hue.
sns.kdeplot(...): Creates a kernel density estimate plot for the 'Age' column, separating the data based on whether 'Survived' is 0 or 1. The 'Pclass' column is used to differentiate the plots.
Cluster Map:

sns.clustermap(...): Creates a cluster map by computing a cross-tabulation between the 'Parch' and 'Survived' columns in the titanic DataFrame.
Line Plot:

sns.lineplot(...): Creates a line plot using the 'year' column as the x-axis and the 'passengers' column as the y-axis. The data is derived from the flights DataFrame, grouped by 'year' and summed.
